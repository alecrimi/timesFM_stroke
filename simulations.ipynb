{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2157e46b-22f6-48ab-8c8c-9cda79e8ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import timesfm\n",
    "import pandas as pd\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error, mean_absolute_scaled_error, mean_absolute_error\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.statsforecast import StatsForecastAutoARIMA, StatsForecastAutoETS\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from scipy.stats import f, pearsonr, ttest_ind\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"possible convergence problem\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59e48d9-7b0f-4bdf-a26c-185ca7e3e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|████████████████████████████████████████████████████| 3/3 [00:00<00:00, 21845.33it/s]\n"
     ]
    }
   ],
   "source": [
    "##Initialize foundation model without fine tuning\n",
    "HORIZON = 1\n",
    "tfm = timesfm.TimesFm(\n",
    "      hparams=timesfm.TimesFmHparams(\n",
    "          backend=\"gpu\",\n",
    "          per_core_batch_size=32,\n",
    "          horizon_len=HORIZON,\n",
    "      ),\n",
    "      checkpoint=timesfm.TimesFmCheckpoint(\n",
    "          huggingface_repo_id=\"google/timesfm-1.0-200m-pytorch\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91e6d00d-4098-4867-9232-501ca840cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/mb-BCA/pyMOU.git@master  --no-deps\n",
    "import pymou as pm\n",
    "import pymou.tools as pmt\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Test inhibitory excitatory\n",
    "# =============================================\n",
    "def generate_test_data_OU(N):\n",
    "    \"\"\"\n",
    "    I changed this.\n",
    "    Generates test data with clear X→Y→Z structure and signed influences\n",
    "    X → Y (positive influence)\n",
    "    Y → Z (negative influence)\n",
    "    \"\"\"\n",
    "    n = 100  # More points for better statistics\n",
    "    dt = 0.1\n",
    "    sigma = 0.01  # Increased noise for more realistic scenario\n",
    "    d = 0.8 #density\n",
    "\n",
    "    # Clear causal structure:\n",
    "    # X → Y , Y → Z\n",
    "    # Negative here means exitatory, because it is negated later again\n",
    "    theta = np.array([\n",
    "        [1.0, 0.0, 0.0],    #\n",
    "        [-5.0, 1.0, 0.0],   # Y depends on X\n",
    "        [0.0, 5.0, 1.0]     # Z depends on Y\n",
    "    ])\n",
    "\n",
    "\n",
    "    theta = pmt.make_rnd_connectivity(N, density=d, w_min=-1/N/d, w_max=1/N/d)\n",
    "\n",
    "\n",
    "    X = np.zeros((n, N))\n",
    "\n",
    "    # Generate the process\n",
    "    for t in range(1, n):\n",
    "        X[t] = X[t-1] + dt * (-theta @ X[t-1]) + np.sqrt(dt) * sigma * np.random.randn(N)\n",
    "\n",
    "    #return pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "    return X, theta\n",
    "\n",
    "# =============================================\n",
    "# TimesFM FORECAST FUNCTION\n",
    "# =============================================\n",
    "def timesfm_forecast(series_data, horizon):\n",
    "    \"\"\"\n",
    "    TimesFM forecasting function - replace with actual tfm.forecast call\n",
    "    \"\"\"\n",
    "    if hasattr(series_data, 'values'):\n",
    "        data = series_data.values.flatten()\n",
    "    else:\n",
    "        data = np.array(series_data).flatten()\n",
    "\n",
    "    if len(data) > 1:\n",
    "        forecast = np.zeros(horizon)\n",
    "        for i in range(horizon):\n",
    "            #forecast[i] = mean_val + rho**i * (last_val - mean_val)\n",
    "\n",
    "            forecast[i] = np.squeeze(tfm.forecast([data], freq=[0])[0])\n",
    "\n",
    "    else:\n",
    "        forecast = np.full(horizon, np.mean(data))\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1565016-48d8-422d-a697-6bae639ae619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IMPROVED CAUSALITY TEST RESULTS ===\n",
      "\n",
      "Testing 1 → 0\n",
      "[Enhanced] not significant\n",
      "  p=0.0850, effect=0.036 (excitatory)\n",
      "  optimal lag=3\n",
      "  residual analysis: corr=-0.007\n",
      "[Granger] not significant\n",
      "  p=0.0692, effect=0.037 (excitatory)\n",
      "\n",
      "Testing 2 → 0\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def enhanced_timesfm_causality(target_series, covariate_series, max_lag=5, alpha=0.01, \n",
    "                              horizon=1, window_size=30, correction_method='fdr_bh'):\n",
    "    \"\"\"\n",
    "    Enhanced causality detection using TimesFM residual analysis combined with traditional methods\n",
    "    with multiple test correction for multiple lags\n",
    "    \"\"\"\n",
    "    target = target_series.flatten()\n",
    "    covar = covariate_series.flatten()\n",
    "    min_len = min(len(target), len(covar))\n",
    "    target = target[:min_len]\n",
    "    covar = covar[:min_len]\n",
    "\n",
    "    # Default return value\n",
    "    default_return = {\n",
    "        'p_value': 1.0,\n",
    "        'p_value_corrected': 1.0,\n",
    "        'avg_coefficient': 0.0,\n",
    "        'effect_type': 'none',\n",
    "        'significant': False,\n",
    "        'significant_corrected': False,\n",
    "        'optimal_lag': None,\n",
    "        'confidence_interval': None,\n",
    "        'residual_analysis': None,\n",
    "        'traditional_analysis': None,\n",
    "        'correction_method': correction_method\n",
    "    }\n",
    "\n",
    "    # PART 1: TimesFM Residual Analysis with multiple test correction\n",
    "    residual_results = []\n",
    "    p_values_residual = []\n",
    "    \n",
    "    try:\n",
    "        # Get TimesFM predictions for the entire series\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        # Rolling predictions using TimesFM\n",
    "        for i in range(window_size, len(target) - 1):\n",
    "            train_window = target[i-window_size:i]\n",
    "            pred = timesfm_forecast(train_window, horizon)\n",
    "            predictions.append(pred[0])\n",
    "            actuals.append(target[i])\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        actuals = np.array(actuals)\n",
    "        residuals = actuals - predictions\n",
    "\n",
    "        # Test if lagged covariates explain residuals\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            if len(residuals) < lag + 10:\n",
    "                continue\n",
    "\n",
    "            # Align residuals with lagged covariates\n",
    "            residuals_aligned = residuals[lag:]\n",
    "            cov_lagged = covar[window_size + lag:window_size + lag + len(residuals_aligned)]\n",
    "\n",
    "            if len(cov_lagged) != len(residuals_aligned):\n",
    "                min_len_align = min(len(cov_lagged), len(residuals_aligned))\n",
    "                residuals_aligned = residuals_aligned[:min_len_align]\n",
    "                cov_lagged = cov_lagged[:min_len_align]\n",
    "\n",
    "            if len(residuals_aligned) < 10:\n",
    "                continue\n",
    "\n",
    "            # Test correlation between lagged covariate and residuals\n",
    "            if np.std(residuals_aligned) > 0 and np.std(cov_lagged) > 0:\n",
    "                correlation, p_val_corr = pearsonr(cov_lagged, residuals_aligned)\n",
    "                p_values_residual.append(p_val_corr)\n",
    "\n",
    "                # Also test with linear regression\n",
    "                reg = LinearRegression().fit(cov_lagged.reshape(-1, 1), residuals_aligned)\n",
    "                r2_score = reg.score(cov_lagged.reshape(-1, 1), residuals_aligned)\n",
    "\n",
    "                residual_results.append({\n",
    "                    'lag': lag,\n",
    "                    'correlation': correlation,\n",
    "                    'correlation_p_value': p_val_corr,\n",
    "                    'r2_residual_explained': r2_score,\n",
    "                    'covariate_coeff': reg.coef_[0],\n",
    "                    'n_samples': len(residuals_aligned)\n",
    "                })\n",
    "\n",
    "        # Apply multiple test correction to residual p-values\n",
    "        if p_values_residual:\n",
    "            reject_res, pvals_corrected_res, _, _ = multipletests(\n",
    "                p_values_residual, alpha=alpha, method=correction_method\n",
    "            )\n",
    "            \n",
    "            # Update residual results with corrected p-values\n",
    "            for i, result in enumerate(residual_results):\n",
    "                result['correlation_p_value_corrected'] = pvals_corrected_res[i]\n",
    "                result['significant_corrected'] = reject_res[i]\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"TimesFM residual analysis failed: {e}\")\n",
    "\n",
    "    # PART 2: Traditional Granger-like analysis with AIC optimization and multiple test correction\n",
    "    aic_values = []\n",
    "    p_values_traditional = []\n",
    "    traditional_results_by_lag = {}\n",
    "    \n",
    "    for lag in range(1, max_lag+1):\n",
    "        n_samples = len(target) - lag - 1\n",
    "        if n_samples < 10:  # Minimum samples required\n",
    "            continue\n",
    "\n",
    "        # Prepare data with proper dimension handling\n",
    "        y = target[lag+1:]\n",
    "        X_target = np.column_stack([target[i:i+n_samples] for i in range(lag, 0, -1)])\n",
    "        X_covar = np.column_stack([covar[i:i+n_samples] for i in range(lag, 0, -1)])\n",
    "\n",
    "        try:\n",
    "            X = np.hstack([X_target, X_covar])\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            pred = model.predict(X)\n",
    "            rss = np.sum((y - pred)**2)\n",
    "            if rss > 0:  # Avoid division by zero\n",
    "                aic = 2*(lag*2) + min_len*np.log(rss/min_len)\n",
    "                aic_values.append((lag, aic))\n",
    "                \n",
    "                # Calculate p-value for this specific lag\n",
    "                X_restricted = X_target\n",
    "                X_full = np.hstack([X_target, X_covar])\n",
    "\n",
    "                # Fit models\n",
    "                model_restricted = LinearRegression().fit(X_restricted, y)\n",
    "                model_full = LinearRegression().fit(X_full, y)\n",
    "\n",
    "                # Calculate statistics\n",
    "                rss_restricted = np.sum((y - model_restricted.predict(X_restricted))**2)\n",
    "                rss_full = np.sum((y - model_full.predict(X_full))**2)\n",
    "\n",
    "                # F-test\n",
    "                n = len(y)\n",
    "                k = lag\n",
    "                if rss_full > 0 and (rss_restricted - rss_full) > 0:\n",
    "                    f_stat = ((rss_restricted - rss_full)/k) / (rss_full/(n - 2*k - 1))\n",
    "                    p_val = 1 - f.cdf(f_stat, k, n - 2*k - 1)\n",
    "                else:\n",
    "                    f_stat, p_val = 0, 1.0\n",
    "                    \n",
    "                p_values_traditional.append(p_val)\n",
    "\n",
    "                # Get effect direction\n",
    "                covar_coeffs = model_full.coef_[lag:2*lag]\n",
    "                avg_effect = np.median(covar_coeffs) if len(covar_coeffs) > 0 else 0.0\n",
    "\n",
    "                # Store results by lag\n",
    "                traditional_results_by_lag[lag] = {\n",
    "                    'p_value': p_val,\n",
    "                    'avg_coefficient': avg_effect,\n",
    "                    'f_statistic': f_stat,\n",
    "                    'aic': aic\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Apply multiple test correction to traditional p-values\n",
    "    corrected_traditional_results = []\n",
    "    if p_values_traditional:\n",
    "        reject_trad, pvals_corrected_trad, _, _ = multipletests(\n",
    "            p_values_traditional, alpha=alpha, method=correction_method\n",
    "        )\n",
    "        \n",
    "        # Update traditional results with corrected p-values\n",
    "        for i, lag in enumerate(range(1, max_lag+1)):\n",
    "            if lag in traditional_results_by_lag:\n",
    "                result = traditional_results_by_lag[lag].copy()\n",
    "                result['p_value_corrected'] = pvals_corrected_trad[i]\n",
    "                result['significant_corrected'] = reject_trad[i]\n",
    "                corrected_traditional_results.append((lag, result))\n",
    "\n",
    "    if not aic_values and not residual_results:\n",
    "        return default_return\n",
    "\n",
    "    # Combine results from both methods\n",
    "    best_residual_result = None\n",
    "    if residual_results:\n",
    "        # Find the best residual result using corrected p-values\n",
    "        best_residual_result = min(residual_results, \n",
    "                                 key=lambda x: x.get('correlation_p_value_corrected', x['correlation_p_value']))\n",
    "\n",
    "    traditional_result = None\n",
    "    best_lag_traditional = None\n",
    "    if corrected_traditional_results:\n",
    "        # Find the best traditional result using corrected p-values\n",
    "        best_lag_traditional = min(corrected_traditional_results, \n",
    "                                 key=lambda x: x[1].get('p_value_corrected', x[1]['p_value']))\n",
    "        traditional_result = best_lag_traditional[1]\n",
    "\n",
    "    # Use the method that gives the most significant result\n",
    "    final_p_value = 1.0\n",
    "    final_p_value_corrected = 1.0\n",
    "    final_avg_coeff = 0.0\n",
    "    final_effect_type = 'none'\n",
    "    final_significant = False\n",
    "    final_significant_corrected = False\n",
    "    final_optimal_lag = None\n",
    "\n",
    "    # Determine which method to use based on corrected significance\n",
    "    if best_residual_result and traditional_result:\n",
    "        residual_p_corrected = best_residual_result.get('correlation_p_value_corrected', best_residual_result['correlation_p_value'])\n",
    "        traditional_p_corrected = traditional_result.get('p_value_corrected', traditional_result['p_value'])\n",
    "        \n",
    "        if residual_p_corrected < traditional_p_corrected:\n",
    "            final_p_value = best_residual_result['correlation_p_value']\n",
    "            final_p_value_corrected = residual_p_corrected\n",
    "            final_avg_coeff = best_residual_result['covariate_coeff']\n",
    "            final_optimal_lag = best_residual_result['lag']\n",
    "            final_significant_corrected = best_residual_result.get('significant_corrected', False)\n",
    "        else:\n",
    "            final_p_value = traditional_result['p_value']\n",
    "            final_p_value_corrected = traditional_p_corrected\n",
    "            final_avg_coeff = traditional_result['avg_coefficient']\n",
    "            final_optimal_lag = best_lag_traditional[0]\n",
    "            final_significant_corrected = traditional_result.get('significant_corrected', False)\n",
    "            \n",
    "    elif best_residual_result:\n",
    "        final_p_value = best_residual_result['correlation_p_value']\n",
    "        final_p_value_corrected = best_residual_result.get('correlation_p_value_corrected', final_p_value)\n",
    "        final_avg_coeff = best_residual_result['covariate_coeff']\n",
    "        final_optimal_lag = best_residual_result['lag']\n",
    "        final_significant_corrected = best_residual_result.get('significant_corrected', False)\n",
    "        \n",
    "    elif traditional_result:\n",
    "        final_p_value = traditional_result['p_value']\n",
    "        final_p_value_corrected = traditional_result.get('p_value_corrected', final_p_value)\n",
    "        final_avg_coeff = traditional_result['avg_coefficient']\n",
    "        final_optimal_lag = best_lag_traditional[0]\n",
    "        final_significant_corrected = traditional_result.get('significant_corrected', False)\n",
    "\n",
    "    # Determine effect type and significance\n",
    "    final_significant = final_p_value < alpha\n",
    "    final_effect_type = 'excitatory' if final_avg_coeff > 0 else 'inhibitory' if final_avg_coeff < 0 else 'none'\n",
    "\n",
    "    # Bootstrap confidence interval for the final coefficient\n",
    "    final_ci = None\n",
    "    if final_significant_corrected and len(target) > window_size and final_optimal_lag is not None:\n",
    "        n_boot = 200\n",
    "        boot_effects = []\n",
    "        for _ in range(n_boot):\n",
    "            try:\n",
    "                # Bootstrap sample\n",
    "                boot_idx = np.random.choice(len(target) - window_size, size=len(target) - window_size, replace=True)\n",
    "                boot_target = target[window_size:][boot_idx]\n",
    "                boot_covar = covar[window_size:][boot_idx]\n",
    "\n",
    "                if len(boot_target) > final_optimal_lag:\n",
    "                    y_boot = boot_target[final_optimal_lag:]\n",
    "                    x_boot = boot_covar[:-final_optimal_lag] if final_optimal_lag > 0 else boot_covar\n",
    "\n",
    "                    if len(y_boot) == len(x_boot) and len(y_boot) > 0:\n",
    "                        boot_reg = LinearRegression().fit(x_boot.reshape(-1, 1), y_boot)\n",
    "                        boot_effects.append(boot_reg.coef_[0])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(boot_effects) > 0:\n",
    "            final_ci = (np.percentile(boot_effects, 2.5), np.percentile(boot_effects, 97.5))\n",
    "\n",
    "    return {\n",
    "        'p_value': final_p_value,\n",
    "        'p_value_corrected': final_p_value_corrected,\n",
    "        'avg_coefficient': final_avg_coeff,\n",
    "        'effect_type': final_effect_type,\n",
    "        'significant': final_significant,\n",
    "        'significant_corrected': final_significant_corrected,\n",
    "        'optimal_lag': final_optimal_lag,\n",
    "        'confidence_interval': final_ci,\n",
    "        'residual_analysis': best_residual_result,\n",
    "        'traditional_analysis': traditional_result,\n",
    "        'correction_method': correction_method\n",
    "    }\n",
    "\n",
    "\n",
    "def classical_granger_test_aic(target_series, covariate_series, max_lag=5, alpha=0.01):\n",
    "    \"\"\"Granger causality with AIC-based lag selection (no multiple testing).\"\"\"\n",
    "    target = target_series.flatten()\n",
    "    covar = covariate_series.flatten()\n",
    "    min_len = min(len(target), len(covar))\n",
    "    target = target[:min_len]\n",
    "    covar = covar[:min_len]\n",
    "\n",
    "    default_return = {\n",
    "        'p_value': 1.0,\n",
    "        'effect_type': 'none',\n",
    "        'significant': False,\n",
    "        'avg_coeff': 0.0,\n",
    "        'lag': None\n",
    "    }\n",
    "\n",
    "    # === Step 1: AIC-based lag selection ===\n",
    "    aic_values = []\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        if len(target) < lag + 10:  # Minimum samples\n",
    "            continue\n",
    "\n",
    "        y = target[lag:]\n",
    "        X_target = np.column_stack([target[lag - l:-l if l != 0 else None] for l in range(1, lag + 1)])\n",
    "        X_covar = np.column_stack([covar[lag - l:-l if l != 0 else None] for l in range(1, lag + 1)])\n",
    "\n",
    "        try:\n",
    "            X_full = np.hstack([X_target, X_covar])\n",
    "            model = LinearRegression().fit(X_full, y)\n",
    "            pred = model.predict(X_full)\n",
    "            rss = np.sum((y - pred) ** 2)\n",
    "            aic = 2 * (lag * 2) + min_len * np.log(rss / min_len)  # AIC formula\n",
    "            aic_values.append((lag, aic))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not aic_values:\n",
    "        return default_return\n",
    "\n",
    "    best_lag = min(aic_values, key=lambda x: x[1])[0]  # Lag with lowest AIC\n",
    "\n",
    "    # === Step 2: Granger test ONLY at the best lag ===\n",
    "    y = target[best_lag:]\n",
    "    X_target = np.column_stack([target[best_lag - l:-l if l != 0 else None] for l in range(1, best_lag + 1)])\n",
    "    X_covar = np.column_stack([covar[best_lag - l:-l if l != 0 else None] for l in range(1, best_lag + 1)])\n",
    "\n",
    "    X_restricted = np.hstack([np.ones((len(y), 1)), X_target])  # Restricted model (no covariate)\n",
    "    X_full = np.hstack([X_restricted, X_covar])                 # Full model (with covariate)\n",
    "\n",
    "    try:\n",
    "        model_restricted = LinearRegression().fit(X_restricted, y)\n",
    "        model_full = LinearRegression().fit(X_full, y)\n",
    "\n",
    "        rss_restricted = np.sum((y - model_restricted.predict(X_restricted)) ** 2)\n",
    "        rss_full = np.sum((y - model_full.predict(X_full)) ** 2)\n",
    "\n",
    "        # F-test (Granger causality)\n",
    "        n = len(y)\n",
    "        k = best_lag\n",
    "        if rss_full > 0 and (rss_restricted - rss_full) > 0:\n",
    "            f_stat = ((rss_restricted - rss_full) / k) / (rss_full / (n - 2 * k - 1))\n",
    "            p_val = 1 - f.cdf(f_stat, k, n - 2 * k - 1)\n",
    "        else:\n",
    "            p_val = 1.0\n",
    "\n",
    "        # Effect direction\n",
    "        covar_coeffs = model_full.coef_[best_lag + 1:2 * best_lag + 1]  # Coefficients for covariate lags\n",
    "        avg_coeff = np.median(covar_coeffs) if len(covar_coeffs) > 0 else 0.0\n",
    "        effect_type = 'excitatory' if avg_coeff > 0 else 'inhibitory' if avg_coeff < 0 else 'none'\n",
    "\n",
    "        # === Add CI-based significance check HERE ===\n",
    "        if final_significant and final_ci is not None:\n",
    "          # Only confirm significance if CI doesn't straddle zero\n",
    "          final_significant = (final_ci[0] * final_ci[1] > 0)  # CI doesn't cross zero\n",
    "\n",
    "\n",
    "        return {\n",
    "            'p_value': p_val,\n",
    "            'effect_type': effect_type,\n",
    "            'significant': p_val < alpha,\n",
    "            'avg_coeff': avg_coeff,\n",
    "            'lag': best_lag,\n",
    "            'f_statistic': f_stat if 'f_stat' in locals() else None\n",
    "        }\n",
    "    except:\n",
    "        return default_return\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import f\n",
    "\n",
    "def classical_granger_test_multiple_correction(target_series, covariate_series, max_lag=5, alpha=0.01, correction_method='fdr_bh'):\n",
    "    \"\"\"Granger causality with multiple test correction across all lags.\"\"\"\n",
    "    target = target_series.flatten()\n",
    "    covar = covariate_series.flatten()\n",
    "    min_len = min(len(target), len(covar))\n",
    "    target = target[:min_len]\n",
    "    covar = covar[:min_len]\n",
    "\n",
    "    default_return = {\n",
    "        'p_value': 1.0,\n",
    "        'p_value_corrected': 1.0,\n",
    "        'effect_type': 'none',\n",
    "        'significant': False,\n",
    "        'significant_corrected': False,\n",
    "        'avg_coeff': 0.0,\n",
    "        'lag': None,\n",
    "        'f_statistic': None,\n",
    "        'correction_method': correction_method,\n",
    "        'all_lag_results': []\n",
    "    }\n",
    "\n",
    "    # Collect results for all lags\n",
    "    all_results = []\n",
    "    p_values = []\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        if len(target) < lag + 10:  # Minimum samples\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            y = target[lag:]\n",
    "            X_target = np.column_stack([target[lag - l:-l] for l in range(1, lag + 1)])\n",
    "            X_covar = np.column_stack([covar[lag - l:-l] for l in range(1, lag + 1)])\n",
    "\n",
    "            X_restricted = np.hstack([np.ones((len(y), 1)), X_target])  # Restricted model (no covariate)\n",
    "            X_full = np.hstack([X_restricted, X_covar])                 # Full model (with covariate)\n",
    "\n",
    "            model_restricted = LinearRegression().fit(X_restricted, y)\n",
    "            model_full = LinearRegression().fit(X_full, y)\n",
    "\n",
    "            rss_restricted = np.sum((y - model_restricted.predict(X_restricted)) ** 2)\n",
    "            rss_full = np.sum((y - model_full.predict(X_full)) ** 2)\n",
    "\n",
    "            # F-test (Granger causality)\n",
    "            n = len(y)\n",
    "            k = lag\n",
    "            if rss_full > 0 and (rss_restricted - rss_full) > 0:\n",
    "                f_stat = ((rss_restricted - rss_full) / k) / (rss_full / (n - 2 * k - 1))\n",
    "                p_val = 1 - f.cdf(f_stat, k, n - 2 * k - 1)\n",
    "            else:\n",
    "                f_stat, p_val = 0, 1.0\n",
    "\n",
    "            # Effect direction\n",
    "            covar_coeffs = model_full.coef_[lag + 1:2 * lag + 1]  # Coefficients for covariate lags\n",
    "            avg_coeff = np.median(covar_coeffs) if len(covar_coeffs) > 0 else 0.0\n",
    "            effect_type = 'excitatory' if avg_coeff > 0 else 'inhibitory' if avg_coeff < 0 else 'none'\n",
    "\n",
    "            # Store results for this lag\n",
    "            lag_result = {\n",
    "                'lag': lag,\n",
    "                'p_value': p_val,\n",
    "                'f_statistic': f_stat,\n",
    "                'avg_coeff': avg_coeff,\n",
    "                'effect_type': effect_type,\n",
    "                'significant': p_val < alpha,\n",
    "                'n_samples': n\n",
    "            }\n",
    "            \n",
    "            all_results.append(lag_result)\n",
    "            p_values.append(p_val)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Skip this lag if there's an error\n",
    "            continue\n",
    "\n",
    "    if not all_results:\n",
    "        return default_return\n",
    "\n",
    "    # Apply multiple test correction\n",
    "    p_values_array = np.array(p_values)\n",
    "    if len(p_values_array) > 0:\n",
    "        reject, pvals_corrected, _, _ = multipletests(\n",
    "            p_values_array, alpha=alpha, method=correction_method\n",
    "        )\n",
    "        \n",
    "        # Update results with corrected p-values\n",
    "        for i, result in enumerate(all_results):\n",
    "            result['p_value_corrected'] = pvals_corrected[i]\n",
    "            result['significant_corrected'] = reject[i]\n",
    "\n",
    "    # Find the most significant result (lowest corrected p-value)\n",
    "    best_result = min(all_results, key=lambda x: x.get('p_value_corrected', x['p_value']))\n",
    "    \n",
    "    # Additional CI-based significance check (optional)\n",
    "    final_significant = best_result.get('significant_corrected', best_result['significant'])\n",
    "    \n",
    "    # You could add bootstrap CI calculation here similar to the enhanced function\n",
    "    # if final_significant and best_result['lag'] is not None:\n",
    "    #     # Bootstrap CI code would go here\n",
    "    #     pass\n",
    "\n",
    "    return {\n",
    "        'p_value': best_result['p_value'],\n",
    "        'p_value_corrected': best_result.get('p_value_corrected', best_result['p_value']),\n",
    "        'effect_type': best_result['effect_type'],\n",
    "        'significant': best_result['significant'],\n",
    "        'significant_corrected': best_result.get('significant_corrected', best_result['significant']),\n",
    "        'avg_coeff': best_result['avg_coeff'],\n",
    "        'lag': best_result['lag'],\n",
    "        'f_statistic': best_result['f_statistic'],\n",
    "        'correction_method': correction_method,\n",
    "        'all_lag_results': all_results  # For debugging and comprehensive analysis\n",
    "    }        \n",
    "\n",
    "# =============================================\n",
    "# VISUALIZATION AND TESTING\n",
    "# =============================================\n",
    "def plot_causal_effects(data):\n",
    "    \"\"\"Visualize the generated time series\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    for i, col in enumerate(['x', 'y', 'z']):\n",
    "        axes[i].plot(data[col])\n",
    "        axes[i].set_title(col.upper())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_causality_relationships():\n",
    "    \"\"\"Main testing function with improved methods\"\"\"\n",
    "    N = 10\n",
    "    data, con_mat = generate_test_data_OU(N)\n",
    "    #data, con_mat = simulate_mou_time_series(N, d=0.3, T=100, M=1)\n",
    "    #data = data[0, :, :]\n",
    "    #plot_causal_effects(data)\n",
    "\n",
    "\n",
    "    #from google.colab import drive\n",
    "    # Mount Google Drive\n",
    "    #drive.mount('/content/drive')\n",
    "    # Save to Drive\n",
    "    save_path = 'con_mat.npy'  # Change to your desired path\n",
    "    np.save(save_path, con_mat)\n",
    "\n",
    "    split = int(1*len(data))  #Split only to increase difficulty, Causality is not a predictive test\n",
    "    train = data[:split,:]\n",
    "\n",
    "    # Initialize connectivity matrices\n",
    "    enhanced_C = np.zeros((N, N))\n",
    "    granger_C = np.zeros((N, N))\n",
    "\n",
    "    print(\"\\n=== IMPROVED CAUSALITY TEST RESULTS ===\")\n",
    "    save_path_LLM = 'enhanced_C.npy'  # Change to your desired path\n",
    "    save_path_GC = 'granger_C.npy'  # Change to your desired path\n",
    "\n",
    "    # Test all possible connections\n",
    "    for target in range(N):\n",
    "        for src in range(N):\n",
    "            if target == src:\n",
    "                continue  # Skip self-connections\n",
    "\n",
    "            print(f\"\\nTesting {src} → {target}\")\n",
    "\n",
    "            # Enhanced method with TimesFM\n",
    "            enhanced_res = enhanced_timesfm_causality(train[:, target], train[:, src], max_lag=5, alpha=0.05)\n",
    "            enhanced_C[target, src] = (1 if enhanced_res['significant'] else 0) * (1 if enhanced_res['effect_type'] == 'excitatory' else -1)\n",
    "            np.save(save_path_LLM, enhanced_C)\n",
    "            print(f\"[Enhanced] {'SIGNIFICANT' if enhanced_res['significant'] else 'not significant'}\")\n",
    "            print(f\"  p={enhanced_res['p_value']:.4f}, effect={enhanced_res['avg_coefficient']:.3f} ({enhanced_res['effect_type']})\")\n",
    "            print(f\"  optimal lag={enhanced_res.get('optimal_lag', 'N/A')}\")\n",
    "            if enhanced_res['residual_analysis']:\n",
    "                print(f\"  residual analysis: corr={enhanced_res['residual_analysis']['correlation']:.3f}\")\n",
    "\n",
    "            # Granger method\n",
    "            #granger_res = classical_granger_test_aic(train[:, target], train[:, src], alpha=0.01)\n",
    "            granger_res = classical_granger_test_multiple_correction(train[:, target], train[:, src], alpha=0.05)\n",
    "            \n",
    "            granger_C[target, src] = (1 if granger_res['significant']  else 0) * (1 if granger_res['effect_type'] == 'excitatory' else -1)\n",
    "            np.save(save_path_GC, granger_C)\n",
    "            print(f\"[Granger] {'SIGNIFICANT' if granger_res['significant'] else 'not significant'}\")\n",
    "            print(f\"  p={granger_res['p_value']:.4f}, effect={granger_res['avg_coeff']:.3f} ({granger_res['effect_type']})\")\n",
    "\n",
    "    # Compare with ground truth\n",
    "    print(\"\\n=== GROUND TRUTH CONNECTIVITY ===\")\n",
    "    # Create a copy to avoid modifying original\n",
    "    converted = np.copy(con_mat)\n",
    "\n",
    "    # Convert positive to 1, negative to -1\n",
    "    converted[converted > 0] = 1\n",
    "    converted[converted < 0] = -1\n",
    "\n",
    "    # Set diagonal to 0\n",
    "    np.fill_diagonal(converted, 0)\n",
    "    con_mat = np.where(converted != 0, -converted, converted)\n",
    "    print(con_mat)\n",
    "\n",
    "    print(\"\\n=== ENHANCED METHOD CONNECTIVITY ===\")\n",
    "    print(enhanced_C)\n",
    "\n",
    "    diff_LLM = np.sum( (con_mat - enhanced_C) ** 2)\n",
    "    print(diff_LLM)\n",
    "\n",
    "    print(\"\\n=== GRANGER METHOD CONNECTIVITY ===\")\n",
    "    print(granger_C)\n",
    "    diff_GC = np.sum( (con_mat - granger_C) ** 2)\n",
    "    print(diff_GC)\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================\n",
    "#if __name__ == \"__main__\":\n",
    "test_causality_relationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47bc8-19fd-4a2d-bb0e-6a147e7f7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_causality_performance(ground_truth_mat, detected_mat, method_name=\"Method\"):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for binary causality detection\n",
    "    \n",
    "    Parameters:\n",
    "    ground_truth_mat: Ground truth connectivity matrix (N x N)\n",
    "    detected_mat: Detected causality matrix (N x N) - binary (0/1 or -1/0/1)\n",
    "    method_name: Name of the method for reporting\n",
    "    \"\"\"\n",
    "    # Flatten matrices for analysis (excluding diagonals)\n",
    "    gt_flat = ground_truth_mat.flatten()\n",
    "    detected_flat = detected_mat.flatten()\n",
    "    \n",
    "    # Remove diagonal elements (self-connections)\n",
    "    n = ground_truth_mat.shape[0]\n",
    "    mask = ~np.eye(n, dtype=bool).flatten()\n",
    "    gt_flat = gt_flat[mask]\n",
    "    detected_flat = detected_flat[mask]\n",
    "    \n",
    "    # Convert to binary: any non-zero value indicates causality\n",
    "    gt_binary = (gt_flat != 0).astype(int)\n",
    "    detected_binary = (detected_flat != 0).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(gt_binary, detected_binary).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    mcc = matthews_corrcoef(gt_binary, detected_binary)\n",
    "    \n",
    "    # Calculate sign agreement for true positives\n",
    "    sign_agreement = 0\n",
    "    if tp > 0:\n",
    "        # Only consider connections that exist in both ground truth and detection\n",
    "        tp_mask = (gt_binary == 1) & (detected_binary == 1)\n",
    "        # For sign comparison, use original values (not binary)\n",
    "        gt_sign = np.sign(gt_flat[tp_mask])\n",
    "        detected_sign = np.sign(detected_flat[tp_mask])\n",
    "        sign_matches = gt_sign == detected_sign\n",
    "        sign_agreement = np.mean(sign_matches) if len(sign_matches) > 0 else 0\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1_score,\n",
    "        'mcc': mcc,  # Matthews Correlation Coefficient\n",
    "        'sign_agreement': sign_agreement,\n",
    "        'total_connections': len(gt_binary),\n",
    "        'actual_causal_connections': np.sum(gt_binary),\n",
    "        'detected_causal_connections': np.sum(detected_binary),\n",
    "        'confusion_matrix': np.array([[tn, fp], [fn, tp]])\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, method_name):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.title(f'Confusion Matrix - {method_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_comparison(results_list):\n",
    "    \"\"\"Plot comparison of different methods' performance\"\"\"\n",
    "    methods = [res['method'] for res in results_list]\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'mcc']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity', 'MCC']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "        values = [res[metric] for res in results_list]\n",
    "        bars = axes[i].bar(methods, values)\n",
    "        axes[i].set_title(metric_name)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, v in zip(bars, values):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, v + 0.01, \n",
    "                        f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_detailed_results(results):\n",
    "    \"\"\"Print detailed performance results\"\"\"\n",
    "    print(f\"\\n=== {results['method']} PERFORMANCE ===\")\n",
    "    print(f\"True Positives: {results['true_positives']}\")\n",
    "    print(f\"False Positives: {results['false_positives']}\")\n",
    "    print(f\"True Negatives: {results['true_negatives']}\")\n",
    "    print(f\"False Negatives: {results['false_negatives']}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {results['recall']:.4f}\")\n",
    "    print(f\"Specificity: {results['specificity']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"MCC: {results['mcc']:.4f}\")\n",
    "    print(f\"Sign Agreement: {results['sign_agreement']:.4f}\")\n",
    "    print(f\"Total connections: {results['total_connections']}\")\n",
    "    print(f\"Actual causal: {results['actual_causal_connections']}\")\n",
    "    print(f\"Detected causal: {results['detected_causal_connections']}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(f\"Precision: {results['precision']:.3f} (How many detected causalities are real)\")\n",
    "    print(f\"Recall:    {results['recall']:.3f} (How many real causalities are detected)\")\n",
    "    print(f\"Specificity: {results['specificity']:.3f} (How many non-causal are correctly identified)\")\n",
    "\n",
    "def evaluate_causality_performance(ground_truth_mat, enhanced_C, granger_C):\n",
    "    \"\"\"\n",
    "    Main function to evaluate causality detection performance\n",
    "    \"\"\"\n",
    "    # Convert ground truth to the same format as detected matrices\n",
    "    gt_formatted = np.copy(ground_truth_mat)\n",
    "    gt_formatted = np.where(gt_formatted != 0, -gt_formatted, gt_formatted)\n",
    "    np.fill_diagonal(gt_formatted, 0)\n",
    "    \n",
    "    # Calculate performance for each method\n",
    "    enhanced_results = calculate_causality_performance(gt_formatted, enhanced_C, \"Enhanced_TimesFM\")\n",
    "    granger_results = calculate_causality_performance(gt_formatted, granger_C, \"Granger_Causality\")\n",
    "    \n",
    "    # Print detailed results\n",
    "    print_detailed_results(enhanced_results)\n",
    "    print_detailed_results(granger_results)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix(enhanced_results['confusion_matrix'], \"Enhanced TimesFM\")\n",
    "    plot_confusion_matrix(granger_results['confusion_matrix'], \"Granger Causality\")\n",
    "    \n",
    "    # Plot performance comparison\n",
    "    plot_performance_comparison([enhanced_results, granger_results])\n",
    "    \n",
    "    return enhanced_results, granger_results\n",
    "\n",
    "# Example of how to integrate with your existing code\n",
    "def test_causality_relationships_with_evaluation():\n",
    "    \"\"\"Main testing function with performance evaluation\"\"\"\n",
    "    N = 10\n",
    "    data, con_mat = generate_test_data_OU(N)\n",
    "    \n",
    "    # Your existing code to compute enhanced_C and granger_C...\n",
    "    # enhanced_C = ... (from enhanced_timesfm_causality)\n",
    "    # granger_C = ... (from classical_granger_test_aic)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    enhanced_results, granger_results = evaluate_causality_performance(con_mat, enhanced_C, granger_C)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Metric':<15} {'Enhanced':<10} {'Granger':<10}\")\n",
    "    print(f\"{'Accuracy':<15} {enhanced_results['accuracy']:.3f}      {granger_results['accuracy']:.3f}\")\n",
    "    print(f\"{'Precision':<15} {enhanced_results['precision']:.3f}      {granger_results['precision']:.3f}\")\n",
    "    print(f\"{'Recall':<15} {enhanced_results['recall']:.3f}      {granger_results['recall']:.3f}\")\n",
    "    print(f\"{'F1 Score':<15} {enhanced_results['f1_score']:.3f}      {granger_results['f1_score']:.3f}\")\n",
    "    print(f\"{'False Positives':<15} {enhanced_results['false_positives']:<10} {granger_results['false_positives']:<10}\")\n",
    "    print(f\"{'False Negatives':<15} {enhanced_results['false_negatives']:<10} {granger_results['false_negatives']:<10}\")\n",
    "    \n",
    "    return enhanced_results, granger_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_results, granger_results = test_causality_relationships_with_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
