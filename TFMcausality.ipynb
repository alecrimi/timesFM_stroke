{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcy7u8BOYD0y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhuUy0yaYHE2"
      },
      "source": [
        "Testing effective connectivity from time series prediction using foundation models. The initial case is without fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke61U6fdYCc0"
      },
      "outputs": [],
      "source": [
        "pip install timesfm sktime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zmrxPHoZ68B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import timesfm\n",
        "import pandas as pd\n",
        "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error, mean_absolute_scaled_error, mean_absolute_error\n",
        "from sktime.forecasting.naive import NaiveForecaster\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sktime.forecasting.compose import make_reduction\n",
        "from sktime.forecasting.statsforecast import StatsForecastAutoARIMA, StatsForecastAutoETS\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from scipy.stats import f, pearsonr, ttest_ind\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"possible convergence problem\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG4noJEraUbm"
      },
      "outputs": [],
      "source": [
        "##Initialize foundation model without fine tuning\n",
        "HORIZON = 1\n",
        "tfm = timesfm.TimesFm(\n",
        "      hparams=timesfm.TimesFmHparams(\n",
        "          backend=\"gpu\",\n",
        "          per_core_batch_size=32,\n",
        "          horizon_len=HORIZON,\n",
        "      ),\n",
        "      checkpoint=timesfm.TimesFmCheckpoint(\n",
        "          huggingface_repo_id=\"google/timesfm-1.0-200m-pytorch\"),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVtzDnUqai1o"
      },
      "outputs": [],
      "source": [
        "##Load time series\n",
        "# Don t use it if you run this with synthetic data\n",
        "def load_data(path, verbose = False):\n",
        "    dataset = pd.read_csv(path, sep='\\t', header = None)\n",
        "    if verbose:\n",
        "        print(dataset.columns)\n",
        "        print(len(dataset))\n",
        "        print(dataset.head())\n",
        "    return dataset\n",
        "control_data = load_data('sub-CON001_ses-control_task-rest_space-MNI152NLin2009cAsym_atlas-Schaefer117_timeseries.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2KOXFQVbCQ3"
      },
      "outputs": [],
      "source": [
        "# convert the structure to be read by the model\n",
        "# Don t use it if you run this with synthetic data\n",
        "def split_train_test(data, break_index):\n",
        "    split = data.index > HORIZON -1\n",
        "    return data[split], data[~split]\n",
        "control_data_train, control_data_test = split_train_test(control_data, HORIZON)\n",
        "\n",
        "def convert_to_timefm(data):\n",
        "    data_time_fm = []\n",
        "    for col in data.columns:\n",
        "        data_time_fm += [data[col]]\n",
        "    return data_time_fm\n",
        "control_data_train_for_time_fm = convert_to_timefm(control_data_train)\n",
        "print(np.shape(control_data_train))\n",
        "print(np.shape(control_data_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5IkaiWnc01E"
      },
      "outputs": [],
      "source": [
        "# Individual prediction\n",
        "# freq=[0] means no seasonality\n",
        "predicted = tfm.forecast(control_data_train_for_time_fm, freq=[0] * len(control_data_train.columns))[0]\n",
        "#predicted = tfm.forecast(control_data_train_for_time_fm, freq=[0] * len(control_data_train_for_time_fm.columns))[0] #adding this will check only one series [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3y-YrNSBDR7n"
      },
      "outputs": [],
      "source": [
        "#  Granger-like Causality Methods Using against Granger\n",
        "\n",
        "\n",
        "# Generate test data (your logistic map coupling)\n",
        "def generate_test_data_LM():\n",
        "    n = 100\n",
        "    r = 3.9\n",
        "    coupling_strength = 0.3\n",
        "\n",
        "    X = np.zeros((3, n))\n",
        "    X[0, 0] = 0.1 + np.random.uniform(-0.01, 0.01)\n",
        "    X[1, 0] = 0.2 + np.random.uniform(-0.01, 0.01)\n",
        "    X[2, 0] = 0.3 + np.random.uniform(-0.01, 0.01)\n",
        "\n",
        "    for i in range(1, n):\n",
        "        X[0, i] = r * X[0, i-1] * (1 - X[0, i-1])\n",
        "        X[1, i] = r * X[1, i-1] * (1 - X[1, i-1])\n",
        "        X[2, i] = r * X[2, i-1] * (1 - X[2, i-1])\n",
        "\n",
        "        X[1, i] += coupling_strength * X[0, i-1]  # x -> y\n",
        "        X[2, i] += coupling_strength * X[1, i-1]  # y -> z\n",
        "\n",
        "        X[:, i] = np.clip(X[:, i], 0, 1)\n",
        "\n",
        "    return pd.DataFrame(X.T, columns=['x', 'y', 'z'])\n",
        "\n",
        "\n",
        "def generate_test_data_OU():\n",
        "    \"\"\"\n",
        "    Generates test data using an Ornstein-Uhlenbeck process with causal structure.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Standardized time series data with columns ['X0', 'X1', 'X2']\n",
        "                      where:\n",
        "                      - X1 is influenced by X0\n",
        "                      - X2 is influenced by X1\n",
        "    \"\"\"\n",
        "    n = 200  # Number of time steps\n",
        "    dt = 0.01  # Time step size\n",
        "\n",
        "    # Coupling matrix defining causal relationships\n",
        "    theta = np.array([[1.0, -8.0, 0.0],   # X0 is influenced by X1\n",
        "                      [0.0, 1.0, 0.0],  # X1 is influenced by X2 (strong negative)\n",
        "                      [0.0, 0.0, 1.0]]) # X2 is influenced by X0 (strong negative)\n",
        "\n",
        "    sigma = 0.01  # Noise level\n",
        "\n",
        "    # Initialize time series\n",
        "    X = np.zeros((n, 3))\n",
        "\n",
        "    # Generate OU process with Euler-Maruyama method\n",
        "    for t in range(1, n):\n",
        "        X[t] = X[t-1] + dt * (-theta @ X[t-1]) + np.sqrt(dt) * sigma * np.random.randn(3)\n",
        "\n",
        "    # Standardize the data\n",
        "    means = X.mean(axis=0)\n",
        "    stds = X.std(axis=0)\n",
        "    stds[stds == 0] = 1.0  # Prevent division by zero\n",
        "    X_standardized = (X - means) / stds\n",
        "\n",
        "    return pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
        "\n",
        "\n",
        "\n",
        "# Dummy TimesFM function (replace with your actual tfm)\n",
        "def timesfm_forecast(series_data, horizon):\n",
        "    \"\"\"Replace this with your actual tfm.forecast call\"\"\"\n",
        "    if hasattr(series_data, 'values'):\n",
        "        data = series_data.values.flatten()\n",
        "    else:\n",
        "        data = np.array(series_data).flatten()\n",
        "\n",
        "    if len(data) > 1:\n",
        "        \"\"\"\n",
        "        # Dummy forecast - to replace with tfm.forecast([data], freq=[0])[0]\n",
        "        # For now, simple AR(1) as placeholder\n",
        "        rho = np.corrcoef(data[:-1], data[1:])[0, 1] if np.std(data) > 0 else 0\n",
        "        rho = np.clip(rho, -0.99, 0.99)\n",
        "        last_val = data[-1]\n",
        "        mean_val = np.mean(data)\n",
        "        \"\"\"\n",
        "        forecast = np.zeros(horizon)\n",
        "        for i in range(horizon):\n",
        "            #forecast[i] = mean_val + rho**i * (last_val - mean_val)\n",
        "\n",
        "            forecast[i] = np.squeeze(tfm.forecast([data], freq=[0])[0])\n",
        "            #print(forecast[i])\n",
        "\n",
        "    else:\n",
        "        forecast = np.full(horizon, np.mean(data))\n",
        "\n",
        "    return forecast\n",
        "\n",
        "\n",
        "# ================================\n",
        "# METHOD : TimesFM Residual Analysis\n",
        "# ================================\n",
        "def timesfm_residual_causality(target_series, covariate_series, max_lag=3):\n",
        "    \"\"\"\n",
        "    Use TimesFM to get baseline predictions, then test if covariate information\n",
        "    explains the residuals\n",
        "    \"\"\"\n",
        "    print(\"Method: TimesFM Residual Analysis\")\n",
        "\n",
        "    target_data = np.array(target_series).flatten()\n",
        "    cov_data = np.array(covariate_series).flatten()\n",
        "\n",
        "    min_len = min(len(target_data), len(cov_data))\n",
        "    target_data = target_data[:min_len-1]  # -1 for prediction\n",
        "    cov_data = cov_data[:min_len-1]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Get TimesFM predictions for the entire series\n",
        "    try:\n",
        "        # Rolling predictions\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        window_size = 30\n",
        "        for i in range(window_size, len(target_data)):\n",
        "            train_window = target_data[i-window_size:i]\n",
        "            pred = timesfm_forecast(train_window, HORIZON)\n",
        "            predictions.append(pred[0])\n",
        "            actuals.append(target_data[i])\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        actuals = np.array(actuals)\n",
        "        residuals = actuals - predictions\n",
        "\n",
        "        # Test if lagged covariates explain residuals\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            if len(residuals) < lag + 10:\n",
        "                continue\n",
        "\n",
        "            # Align residuals with lagged covariates\n",
        "            residuals_aligned = residuals[lag:]\n",
        "            cov_lagged = cov_data[window_size:window_size + len(residuals_aligned)]\n",
        "\n",
        "            if len(cov_lagged) != len(residuals_aligned):\n",
        "                min_len_align = min(len(cov_lagged), len(residuals_aligned))\n",
        "                residuals_aligned = residuals_aligned[:min_len_align]\n",
        "                cov_lagged = cov_lagged[:min_len_align]\n",
        "\n",
        "            if len(residuals_aligned) < 10:\n",
        "                continue\n",
        "\n",
        "            # Test correlation between lagged covariate and residuals\n",
        "            if np.std(residuals_aligned) > 0 and np.std(cov_lagged) > 0:\n",
        "                correlation, p_val_corr = pearsonr(cov_lagged, residuals_aligned)\n",
        "\n",
        "                # Also test with linear regression\n",
        "                reg = LinearRegression().fit(cov_lagged.reshape(-1, 1), residuals_aligned)\n",
        "                r2_score = reg.score(cov_lagged.reshape(-1, 1), residuals_aligned)\n",
        "\n",
        "                results.append({\n",
        "                    'lag': lag,\n",
        "                    'correlation': correlation,\n",
        "                    'correlation_p_value': p_val_corr,\n",
        "                    'r2_residual_explained': r2_score,\n",
        "                    'covariate_coeff': reg.coef_[0],\n",
        "                    'n_samples': len(residuals_aligned)\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Residual analysis failed: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ================================\n",
        "# METHOD : Classical Granger Test (Baseline)\n",
        "# ================================\n",
        "def classical_granger_test(target_series, covariate_series, max_lag):\n",
        "    \"\"\"\n",
        "    Classical Granger causality test for comparison\n",
        "    \"\"\"\n",
        "    print(\"Method : Classical Granger Test (Baseline)\")\n",
        "\n",
        "    target_data = np.array(target_series).flatten()\n",
        "    cov_data = np.array(covariate_series).flatten()\n",
        "\n",
        "    min_len = min(len(target_data), len(cov_data))\n",
        "    target_data = target_data[:min_len]\n",
        "    cov_data = cov_data[:min_len]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        if len(target_data) < lag + 20:\n",
        "            continue\n",
        "\n",
        "        # Prepare data\n",
        "        y = target_data[lag:]\n",
        "\n",
        "        # Restricted model: y_t = α + β₁*y_{t-1} + ... + βₖ*y_{t-k} + ε\n",
        "        X_restricted = np.ones((len(y), 1))  # intercept\n",
        "        for l in range(1, lag + 1):\n",
        "            X_restricted = np.column_stack([X_restricted, target_data[lag-l:-l if l > 0 else len(target_data)]])\n",
        "\n",
        "        # Full model: add lagged covariates\n",
        "        X_full = X_restricted.copy()\n",
        "        for l in range(1, lag + 1):\n",
        "            X_full = np.column_stack([X_full, cov_data[lag-l:-l if l > 0 else len(cov_data)]])\n",
        "\n",
        "        try:\n",
        "            # Fit models\n",
        "            reg_restricted = LinearRegression().fit(X_restricted, y)\n",
        "            reg_full = LinearRegression().fit(X_full, y)\n",
        "\n",
        "            # Calculate RSS\n",
        "            pred_restricted = reg_restricted.predict(X_restricted)\n",
        "            pred_full = reg_full.predict(X_full)\n",
        "\n",
        "            rss_restricted = np.sum((y - pred_restricted) ** 2)\n",
        "            rss_full = np.sum((y - pred_full) ** 2)\n",
        "\n",
        "            # F-test\n",
        "            n = len(y)\n",
        "            k = lag  # number of additional parameters\n",
        "\n",
        "            if rss_full > 0:\n",
        "                f_stat = ((rss_restricted - rss_full) / k) / (rss_full / (n - 2*lag - 1))\n",
        "                from scipy.stats import f\n",
        "                p_val = 1 - f.cdf(f_stat, k, n - 2*lag - 1) if f_stat > 0 else 1.0\n",
        "            else:\n",
        "                f_stat = 0\n",
        "                p_val = 1.0\n",
        "\n",
        "            # Extract covariate coefficients\n",
        "            cov_coeffs = reg_full.coef_[-lag:] if len(reg_full.coef_) >= lag else []\n",
        "\n",
        "            results.append({\n",
        "                'lag': lag,\n",
        "                'f_statistic': f_stat,\n",
        "                'p_value': p_val,\n",
        "                'covariate_coeffs': cov_coeffs.tolist(),\n",
        "                'rss_improvement': rss_restricted - rss_full,\n",
        "                'n_samples': n\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Classical test lag {lag} failed: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ================================\n",
        "# Main Execution\n",
        "# ================================\n",
        "print(\"=== Testing Multiple Granger-like Causality Methods with TimesFM ===\\n\")\n",
        "\n",
        "# Generate test data\n",
        "control_data = generate_test_data_OU() #Generater data with Logistic map causality\n",
        "split_point = int(0.8 * len(control_data))\n",
        "control_data_train = control_data.iloc[:split_point]\n",
        "control_data_test = control_data.iloc[split_point:]\n",
        "\n",
        "from statsmodels.tsa.api import VAR\n",
        "model = VAR(control_data_train)\n",
        "lag_order_results = model.select_order(maxlags=15)\n",
        "lags = lag_order_results.aic  # or use bic or hqic\n",
        "print(f\"Optimal lag (AIC): {lags}\")\n",
        "\n",
        "\n",
        "# Test all methods for known relationships\n",
        "#test_relationships = [('x', 'y'), ('y', 'z'), ('z', 'x')]  # Include a null case\n",
        "test_relationships = [('x', 'y'), ('y','x') , ('y', 'z'),('z', 'y'), ('z', 'x'),('x', 'z')]\n",
        "\n",
        "for covariate_name, target_name in test_relationships:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {covariate_name} -> {target_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    target_series = control_data_train[target_name]\n",
        "    covariate_series = control_data_train[covariate_name]\n",
        "\n",
        "\n",
        "    # Method : LLM Residual analysis\n",
        "    try:\n",
        "        results3 = timesfm_residual_causality(target_series, covariate_series,lags)\n",
        "        if results3:\n",
        "            best_result = min(results3, key=lambda x: x['correlation_p_value'])\n",
        "            print(f\"Method - Best result: Lag={best_result['lag']}, p={best_result['correlation_p_value']:.4f}, \"\n",
        "                  f\"corr={best_result['correlation']:.4f}\")\n",
        "        else:\n",
        "            print(\"Method LLM - No results\")\n",
        "    except Exception as e:\n",
        "        print(f\"Method  LLM failed: {e}\")\n",
        "\n",
        "    # Method : Classical Granger (baseline)\n",
        "    try:\n",
        "        results4 = classical_granger_test(target_series, covariate_series,lags)\n",
        "        if results4:\n",
        "            best_result = min(results4, key=lambda x: x['p_value'])\n",
        "            print(f\"Method Granger - Best result: Lag={best_result['lag']}, p={best_result['p_value']:.4f}, \"\n",
        "                  f\"F={best_result['f_statistic']:.3f}\")\n",
        "        else:\n",
        "            print(\"Method Granger - No results\")\n",
        "    except Exception as e:\n",
        "        print(f\"Method Granger failed: {e}\")\n",
        "\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}